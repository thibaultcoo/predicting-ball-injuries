{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f883a50",
   "metadata": {},
   "source": [
    "# Predicting basketball injuries using ML\n",
    "### Alternative finance project\n",
    "\n",
    "Supervised by Prof. Marcus Frunza - coded by **Soughati Kenza, Henry-Biabaud Briac, Collin Thibault**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae9677e",
   "metadata": {},
   "source": [
    "The aim of this project was to work around machine learning models to predict NBA basketball player injuries for the next game-ahead, using a collection of box score individual statistics from previous games."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c81e1ad",
   "metadata": {},
   "source": [
    "Having already extracted all the data required in a previous notebook {*extraction*}, we now work on implementing machine learning techniques to analyze the sets and work on the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc67ab4f",
   "metadata": {},
   "source": [
    "We import several well-known libraries, and we initialize the booleans that will determine the final pipeline ran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9cf3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f709def",
   "metadata": {},
   "outputs": [],
   "source": [
    "usingPCA = False\n",
    "usingXGBoost = False\n",
    "usingDeep = True\n",
    "useSmote = False\n",
    "useAdasyn = True\n",
    "usingRecursiveElim = True\n",
    "usingFocalLoss = False\n",
    "usingIsolationForest = True\n",
    "usingLightGBM = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b074919",
   "metadata": {},
   "source": [
    "## Features engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15655b82",
   "metadata": {},
   "source": [
    "Because our set are so imbalanced by nature (barely 5% of all individual games result in an injury) we have to rebalance conveniently our data. We perform *resampling* to preserve patterns and only remove what can be considered as close duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339d8874",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.read_csv('stats_full.txt', sep=' ')\n",
    "\n",
    "y_df = stats_df[['Inj After']]\n",
    "x_df = stats_df.drop(columns=['Inj After', 'Season'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3065a59",
   "metadata": {},
   "source": [
    "Splitting our set into training and set sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8259d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_all, x_test, y_all, y_test = train_test_split(x_df, y_df, test_size=0.2, random_state=1)\n",
    "x_all = x_all.drop(columns=['Player', 'Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85522cc",
   "metadata": {},
   "source": [
    "Using isolation forest to detect and remove outliers from out set.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e61d564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "if usingIsolationForest:\n",
    "    iso_forest = IsolationForest(n_estimators=100, max_samples='auto', contamination=0.03, n_jobs=-1, random_state=203)\n",
    "    outliers_pred = iso_forest.fit_predict(x_all)\n",
    "\n",
    "    outliers_cleaned_x_all = x_all[outliers_pred == 1]\n",
    "    outliers_df_scores = y_all[outliers_pred == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c603ca8",
   "metadata": {},
   "source": [
    "## Features selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da003e04",
   "metadata": {},
   "source": [
    "We store a baseline entire selection of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6f489c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(x_all.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caabdc0",
   "metadata": {},
   "source": [
    "Performing PCA to retain features that explain a certain threshold of the total variance from the target variable.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc489a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "if usingPCA:\n",
    "    scaler = StandardScaler()\n",
    "    x_scaled = scaler.fit_transform(x_all[features])\n",
    "\n",
    "    pca = PCA(n_components=0.999)\n",
    "    x_pca = pca.fit_transform(x_scaled)\n",
    "    n_components_chosen = pca.n_components_\n",
    "\n",
    "    components_abs = np.abs(pca.components_)\n",
    "    important_feature_indices = []\n",
    "    chosen_indices = set()\n",
    "\n",
    "    for _ in range(n_components_chosen):\n",
    "        max_index = np.argmax(components_abs)\n",
    "        row, col = divmod(max_index, components_abs.shape[1])\n",
    "        while col in chosen_indices:\n",
    "            components_abs[row, col] = 0\n",
    "            max_index = np.argmax(components_abs)\n",
    "            row, col = divmod(max_index, components_abs.shape[1])\n",
    "        important_feature_indices.append(col)\n",
    "        chosen_indices.add(col)\n",
    "\n",
    "    features_new = [features[i] for i in important_feature_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03703e58",
   "metadata": {},
   "source": [
    "We also try running recursive elimination to select the features that explain variance the most.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e215d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "if usingRecursiveElim:\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(x_all[features])\n",
    "    \n",
    "    classifier = LogisticRegression(max_iter=10000)\n",
    "    rfe = RFE(estimator=classifier, n_features_to_select=int(0.95*len(features)))\n",
    "    \n",
    "    y_all_numpy = y_all.values.ravel() if isinstance(y_all, pd.DataFrame) else y_all.ravel()\n",
    "    \n",
    "    rfe.fit(X_scaled, y_all_numpy)\n",
    "    \n",
    "    feature_mask = rfe.support_\n",
    "    features_new = [feature for feature, selected in zip(features, feature_mask) if selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19dc455",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12b364b",
   "metadata": {},
   "source": [
    "First we split our set into train and validation sets for a better overall performance.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb84471",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(x_all, y_all, test_size=0.2, random_state=1)\n",
    "\n",
    "x_train = x_train[features_new]\n",
    "x_val = x_val[features_new]\n",
    "x_test = x_test[features_new + ['Player']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2a474b",
   "metadata": {},
   "source": [
    "## Gradient boosting machines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87237c9d",
   "metadata": {},
   "source": [
    "LightGBM is a first model we are going to implement.\n",
    "\n",
    "https://lightgbm.readthedocs.io/en/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542f2acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "if usingLightGBM:\n",
    "    scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "    pr_auc_scorer = make_scorer(average_precision_score, response_method='predict_proba')\n",
    "\n",
    "    param_grid = {\n",
    "        'lgbmclassifier__num_leaves': [120],\n",
    "        'lgbmclassifier__reg_alpha': [1.0],\n",
    "        'lgbmclassifier__reg_lambda': [1.0],\n",
    "        'lgbmclassifier__learning_rate': [0.01],\n",
    "        'lgbmclassifier__n_estimators': [80]\n",
    "    }\n",
    "   \n",
    "    if useSmote:\n",
    "        lgbm_pipeline = Pipeline([\n",
    "            ('sampling', SMOTE(random_state=203)),\n",
    "            ('lgbmclassifier', lgb.LGBMClassifier(use_label_encoder=False, eval_metric='logloss', scale_pos_weight=scale_pos_weight))\n",
    "        ])\n",
    "    \n",
    "    if useAdasyn:\n",
    "        lgbm_pipeline = Pipeline([\n",
    "            ('sampling', ADASYN(random_state=203)),\n",
    "            ('lgbmclassifier', lgb.LGBMClassifier(use_label_encoder=False, eval_metric='logloss', scale_pos_weight=scale_pos_weight))\n",
    "        ])\n",
    "\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=203)\n",
    "        \n",
    "    grid_search = GridSearchCV(\n",
    "        lgbm_pipeline,\n",
    "        param_grid,\n",
    "        cv=kf,\n",
    "        scoring='roc_auc',\n",
    "        refit='pr_auc',\n",
    "        n_jobs=-1,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    optimal_params = grid_search.best_params_\n",
    "    best_validation_score = grid_search.best_score_\n",
    "    y_pred_proba_test = grid_search.predict_proba(x_test.drop(columns=['Player']))[:, 1]\n",
    "    test_set_auc_roc = roc_auc_score(y_test, y_pred_proba_test)\n",
    "\n",
    "    print(\"=== Optimal Parameters Found ===\")\n",
    "    for param, value in optimal_params.items():\n",
    "        print(f\"- {param}: {value}\")\n",
    "\n",
    "    print(\"\\n=== Performance Evaluation ===\")\n",
    "    print(f\"AUC-ROC Score (Validation Set): {best_validation_score:.4f}\")\n",
    "    print(f\"AUC-ROC Score (Test Set): {test_set_auc_roc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ad178e",
   "metadata": {},
   "source": [
    "We also try an alternative gradient boosting machine, xgBoost.\n",
    "\n",
    "https://xgboost.readthedocs.io/en/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534f4c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import roc_auc_score, make_scorer, average_precision_score\n",
    "\n",
    "if usingXGBoost:\n",
    "    scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "    pr_auc_scorer = make_scorer(average_precision_score, response_method='predict_proba')\n",
    "\n",
    "    param_grid = {\n",
    "        'xgboost__n_estimators': [135],\n",
    "        'xgboost__max_depth': [15],\n",
    "        'xgboost__learning_rate': [0.1],\n",
    "        'xgboost__subsample': [1.0],\n",
    "        'xgboost__colsample_bytree': [1.0],\n",
    "        'xgboost__gamma': [0.1], \n",
    "        'xgboost__min_child_weight': [1],\n",
    "        'xgboost__reg_alpha': [0.1],\n",
    "        'xgboost__reg_lambda': [0.2],\n",
    "    }\n",
    "    \n",
    "    if useSmote:\n",
    "        xgb_pipeline = ImbPipeline([\n",
    "            ('sampling', SMOTE(random_state=203)),\n",
    "            ('xgboost', XGBClassifier(use_label_encoder=False, eval_metric='logloss', scale_pos_weight=scale_pos_weight))\n",
    "        ])\n",
    "    \n",
    "    if useAdasyn:\n",
    "        xgb_pipeline = ImbPipeline([\n",
    "            ('sampling', ADASYN(random_state=203)),\n",
    "            ('xgboost', XGBClassifier(use_label_encoder=False, eval_metric='logloss', scale_pos_weight=scale_pos_weight))\n",
    "        ])\n",
    "\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=203)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        xgb_pipeline,\n",
    "        param_grid,\n",
    "        cv=kf,\n",
    "        scoring='roc_auc',\n",
    "        refit='pr_auc',\n",
    "        n_jobs=-1,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    optimal_params = grid_search.best_params_\n",
    "    best_validation_score = grid_search.best_score_\n",
    "    y_pred_proba_test = grid_search.predict_proba(x_test.drop(columns=['Player']))[:, 1]\n",
    "    test_set_auc_roc = roc_auc_score(y_test, y_pred_proba_test)\n",
    "\n",
    "    print(\"=== Optimal Parameters Found ===\")\n",
    "    for param, value in optimal_params.items():\n",
    "        print(f\"- {param}: {value}\")\n",
    "\n",
    "    print(\"\\n=== Performance Evaluation ===\")\n",
    "    print(f\"AUC-ROC Score (Validation Set): {best_validation_score:.4f}\")\n",
    "    print(f\"AUC-ROC Score (Test Set): {test_set_auc_roc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf2484f",
   "metadata": {},
   "source": [
    "## Deep learning experimentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb0388e",
   "metadata": {},
   "source": [
    "Now trying to implement a deep learning model to handle class imbalances.\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94837cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def focal_loss(gamma=2.0, alpha=0.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        \n",
    "        epsilon = K.epsilon()\n",
    "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        cross_entropy_pos = -y_true * K.log(y_pred)\n",
    "        cross_entropy_neg = -(1 - y_true) * K.log(1 - y_pred)\n",
    "        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy_pos + (1 - alpha) * K.pow(y_pred, gamma) * cross_entropy_neg\n",
    "        return K.mean(loss, axis=-1)\n",
    "    \n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baaa251",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC, TruePositives, FalsePositives, TrueNegatives, FalseNegatives\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "if usingDeep:\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train['Inj After'].values), y=y_train['Inj After'].values)\n",
    "    class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "    model = Sequential([\n",
    "        Input(shape=x_train.shape[1:]),\n",
    "        Dense(128, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(96, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(64, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(48, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    lr_schedule = ExponentialDecay(initial_learning_rate=1e-3, decay_steps=5000, decay_rate=0.90)\n",
    "    optimizer = Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    if usingFocalLoss:\n",
    "        model.compile(optimizer=optimizer, loss=focal_loss(gamma=2.0, alpha=0.25), metrics=[AUC()])\n",
    "    else:\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[AUC()])\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(x_train, y_train, epochs=12, validation_data=(x_val, y_val), callbacks=[early_stopping], verbose=False, class_weight=class_weight_dict)\n",
    "\n",
    "    test_results = model.evaluate(x_test.drop(columns=['Player']), y_test)\n",
    "    val_pred = model.predict(x_val)\n",
    "    val_auc_roc = roc_auc_score(y_val, val_pred)\n",
    "    print(\"\\n=== Performance Evaluation ===\")\n",
    "    print(f\"AUC-ROC Score (Validation Set): {val_auc_roc:.4f}\")\n",
    "    print(f\"AUC-ROC Score (Test Set): {test_results[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6876317e",
   "metadata": {},
   "source": [
    "The scores indicate a modest but correct ability to discern those highly imbalanced classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8992a38",
   "metadata": {},
   "source": [
    "## Prevent betting on injury-prone valuable players"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdaefcf",
   "metadata": {},
   "source": [
    "The model has a modest ability to predict players with a tendency to get injured at the next game. We therefore introduce  a second test set, more recent and only composed of very recent players statistics. We will run our prediction model and output an injury watch list, with an indicator of how important would the player's absence be. Then, we will scrap the upcoming game and work on giving betting advices based on odds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e7a4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df = pd.read_csv('testing_stats.txt', sep=' ')\n",
    "\n",
    "testing_df = testing_df.drop(columns=['Inj After'])\n",
    "x_testing_df = testing_df.drop(columns=['Date', 'Season'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccc4b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_testing_df.drop(columns=['Player'])[features_new])\n",
    "\n",
    "threshold = 0.35\n",
    "y_pred_binary = (y_pred < threshold).astype(int)\n",
    "\n",
    "testing_df['Prediction'] = y_pred_binary\n",
    "predicted_df = testing_df[testing_df['Prediction'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eee3146",
   "metadata": {},
   "outputs": [],
   "source": [
    "injury_watch = predicted_df[['Player', 'Valuation_Indic']]\n",
    "injury_watch_sorted = injury_watch.sort_values(by='Valuation_Indic', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2b0ee3",
   "metadata": {},
   "source": [
    "## Retrieve the full name and current team of the players from the injury watch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0971c342",
   "metadata": {},
   "source": [
    "We therefore get a more visual indicator on where not to bet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb457d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "players = injury_watch_sorted['Player']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed68bb2",
   "metadata": {},
   "source": [
    "Below we scrap the needed data: retrieving the full names of each player for easier understanding, what game they are supposed to play next, and then an indication on how an injury would impact the game, based on short-term previous performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a3a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for player in players:\n",
    "    base_url = 'https://www.basketball-reference.com/players/'\n",
    "    url = base_url + str(player[0]) + '/' + str(player) + '.html'\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    player_name_tag = soup.select_one('h1 span')\n",
    "    player_name = player_name_tag.text.strip()\n",
    "    \n",
    "    injury_watch_sorted.loc[injury_watch_sorted['Player'] == player, 'Name'] = player_name\n",
    "    \n",
    "    url = base_url + str(player[0]) + '/' + str(player) + '/gamelog/2023'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    table = soup.find(\"table\", {\"id\": \"pgl_basic\"})\n",
    "\n",
    "    columns = [th.getText() for th in table.find(\"thead\").findAll(\"th\")][1:]\n",
    "\n",
    "    data_rows = table.find(\"tbody\").findAll(\"tr\")\n",
    "    data = [[td.getText() for td in data_rows[i].findAll(\"td\")] for i in range(len(data_rows))]\n",
    "\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    target_date = pd.Timestamp('2023-03-27')\n",
    "    after_target_date = df[df['Date'] > target_date].iloc[0]\n",
    "\n",
    "    injury_watch_sorted.loc[injury_watch_sorted['Player'] == player, 'Current team'] = after_target_date['Tm']\n",
    "    injury_watch_sorted.loc[injury_watch_sorted['Player'] == player, 'Next opponent'] = after_target_date['Opp']\n",
    "    injury_watch_sorted.loc[injury_watch_sorted['Player'] == player, 'Next game date'] = after_target_date['Date']\n",
    "    \n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d42c0f",
   "metadata": {},
   "source": [
    "Transforming the ouput to provide a more straightforward metric for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d55f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    injury_watch_sorted['Valuation_Indic'] < 35,\n",
    "    (injury_watch_sorted['Valuation_Indic'] >= 35) & (injury_watch_sorted['Valuation_Indic'] < 50),\n",
    "    (injury_watch_sorted['Valuation_Indic'] >= 50) & (injury_watch_sorted['Valuation_Indic'] < 75),\n",
    "    injury_watch_sorted['Valuation_Indic'] >= 75]\n",
    "\n",
    "labels = ['Poor', 'Average', 'Significant', 'Critical']\n",
    "\n",
    "injury_watch_sorted['Injury influence on game'] = np.select(conditions, labels, default='Unknown')\n",
    "injury_watch_sorted = injury_watch_sorted.drop(columns=['Player', 'Valuation_Indic']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b88fd5a",
   "metadata": {},
   "source": [
    "The resulting dataframe shows the user a list of future players and games to avoid betting on, giving a relative risk of the player getting injured during the game. The theoretical influence his injury would have is also listed for context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422e71dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "injury_watch_sorted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
